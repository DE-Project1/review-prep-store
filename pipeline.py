# pipeline.py
from googled_fetch_data import fetch_place_info, fetch_reviews
from preprocess.deduplicate_places import deduplicate_places
from preprocess.filter_columns import filter_columns
from preprocess.clean_text import clean_text
from preprocess.extract_nouns import extract_nouns_from_reviews
from preprocess.validate_reviews import validate_reviews
from tests.csv_to_json import convert_to_json
from db.init_collections import init_collections
from db.insert_data import insert_data
from utils.logger import get_logger

logger = get_logger("pipeline")

def run_pipeline(region_csv_path: str):
    try:
        logger.info("ğŸš€ Step 1: Google Driveì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        df_place_info_raw = fetch_place_info()
        df_reviews_raw = fetch_reviews()
        logger.debug(f"ğŸ“¦ Place info shape: {df_place_info_raw.shape}, Reviews shape: {df_reviews_raw.shape}")
    except Exception as e:
        logger.error(f"âŒ Google Drive ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ” Step 2: ì¤‘ë³µ place_id ì²˜ë¦¬ ë° ë¦¬ë·° í•„í„°ë§ ì¤‘...")
        df_place_dedup, df_reviews_dedup, _ = deduplicate_places(df_place_info_raw, df_reviews_raw)
        logger.debug(f"ğŸ§¹ Deduplicated places: {df_place_dedup.shape}, Deduplicated reviews: {df_reviews_dedup.shape}")
    except Exception as e:
        logger.error(f"âŒ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ“‘ Step 3: ì»¬ëŸ¼ í•„í„°ë§...")
        df_place_filtered, df_reviews_filtered = filter_columns(df_place_dedup, df_reviews_dedup)
        logger.debug(f"ğŸ“„ Filtered place columns: {df_place_filtered.columns.tolist()}")
        logger.debug(f"ğŸ“ Filtered review columns: {df_reviews_filtered.columns.tolist()}")
    except Exception as e:
        logger.error(f"âŒ ì»¬ëŸ¼ í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ§¼ Step 4: ë¦¬ë·° í…ìŠ¤íŠ¸ í´ë Œì§•...")
        logger.debug(f"ğŸ”¢ í´ë Œì§• ì „ ë¦¬ë·° ìˆ˜: {len(df_reviews_filtered)}")
        df_reviews_cleaned = clean_text(df_reviews_filtered)
        logger.debug(f"ğŸ”¢ í´ë Œì§• í›„ ë¦¬ë·° ìˆ˜: {len(df_reviews_cleaned)}")
        logger.debug(f"âœ‚ï¸ Cleaned review ì˜ˆì‹œ: {df_reviews_cleaned['content'].iloc[0]}")
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ í´ë Œì§• ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ§  Step 5: ëª…ì‚¬ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì œê±°...")
        logger.debug(f"ğŸ”¢ ëª…ì‚¬ ì¶”ì¶œ ì „ ë¦¬ë·° ìˆ˜: {len(df_reviews_cleaned)}")
        df_reviews_nouns = extract_nouns_from_reviews(df_reviews_cleaned)
        logger.debug(f"ğŸ”¢ ëª…ì‚¬ ì¶”ì¶œ í›„ ë¦¬ë·° ìˆ˜: {len(df_reviews_nouns)}")
        logger.debug(f"ğŸ”  Noun extraction ì˜ˆì‹œ: {df_reviews_nouns['content_nouns'].iloc[0]}")
    except Exception as e:
        logger.error(f"âŒ ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("âœ… Step 6: ìœ íš¨ ë¦¬ë·°ë§Œ í•„í„°ë§...")
        logger.debug(f"ğŸ”¢ ìœ íš¨ì„± ê²€ì¦ ì „ ë¦¬ë·° ìˆ˜: {len(df_reviews_nouns)}")
        df_reviews_valid = validate_reviews(df_reviews_nouns)
        logger.debug(f"ğŸ”¢ ìœ íš¨ì„± ê²€ì¦ í›„ ë¦¬ë·° ìˆ˜: {len(df_reviews_valid)}")
        logger.debug(f"ğŸ§ª ìœ íš¨ ë¦¬ë·° ìˆ˜: {len(df_reviews_valid)}")
    except Exception as e:
        logger.error(f"âŒ ìœ íš¨ ë¦¬ë·° í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ“¦ Step 7: JSON ë³€í™˜ ì¤‘...")
        place_info_json, reviews_json = convert_to_json(df_place_filtered, df_reviews_valid)
        logger.debug(f"ğŸ§¾ JSON ìƒ˜í”Œ place_info: {place_info_json[0]}")
        logger.debug(f"ğŸ§¾ JSON ìƒ˜í”Œ review: {reviews_json[0]}")
    except Exception as e:
        logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸŒ Step 8: ì§€ì—­êµ¬ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”...")
        init_collections(region_csv_path)
        logger.debug(f"ğŸ“ CSV ê²½ë¡œ: {region_csv_path}")
    except Exception as e:
        logger.error(f"âŒ ì§€ì—­êµ¬ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    try:
        logger.info("ğŸ“¤ Step 9: MongoDBì— ë°ì´í„° ì ì¬ ì¤‘...")
        insert_data(place_info_json, reviews_json)
        logger.debug("âœ… ë°ì´í„° ì ì¬ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ MongoDB ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {e}")
        return

    logger.info("ğŸ‰ ëª¨ë“  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ë° MongoDB ì €ì¥ ì™„ë£Œ!")

# ì‹¤í–‰ (python pipeline.py)
if __name__ == "__main__":
    run_pipeline("data/adm_dong_list.csv")












